{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction in the Greek Web\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we extract the texts from the folders..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import stemmer as st\n",
    "def process_word(word):\n",
    "    \"\"\"detone and stem word\n",
    "    \"\"\"\n",
    "    new_word = []\n",
    "    tones = { u'Ά' : u'A' , u'Έ': u'Ε', u'Ί': u'Ι', u'Ϊ': u'Ι',\n",
    "             u'Ύ': u'Υ', u'Ϋ': u'Υ', u'Ό' : u'Ο', u'Ή': u'Η', u'Ώ': u'Ω'}\n",
    "    for letter in word:\n",
    "        try:\n",
    "            new_word.append(tones[letter])\n",
    "        except KeyError:\n",
    "            new_word.append(letter)\n",
    "    detoned = ''.join(l for l in new_word)\n",
    "    return st.stem(detoned)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def process_text(data):\n",
    "    with open('dataset/greekstopwords.txt', 'r') as fp:\n",
    "        stopwords = []\n",
    "        for line in fp:\n",
    "            stopwords.append(line.strip().decode('utf-8').upper())\n",
    "    for domain in data.keys():\n",
    "        text = data[domain]\n",
    "        # remove punctuation\n",
    "        punctuation = set(string.punctuation)    \n",
    "        doc = ''.join([w for w in text if w not in punctuation])\n",
    "        # remove stopwords\n",
    "        doc = [w for w in doc.split() if w not in stopwords]\n",
    "        doc = ' '.join(process_word(w) for w in doc)\n",
    "        data[domain] = doc\n",
    "    return data        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "if os.path.isfile('cache/processed_text.pickle'):\n",
    "    with open('cache/processed_text.pickle', 'rb') as pfile:\n",
    "        text_data = pickle.load(pfile)\n",
    "        print \"loaded from pickle\"\n",
    "else:\n",
    "    filenames = os.listdir('dataset/hosts')\n",
    "    raw_text = {}\n",
    "    for zipfilename in filenames:\n",
    "        with zipfile.ZipFile('dataset/hosts/'+zipfilename) as z:\n",
    "            text = \"\"\n",
    "            for filename in z.namelist():\n",
    "                if not os.path.isdir(filename):\n",
    "                    with z.open(filename) as f:\n",
    "                        for line in f:\n",
    "                            text += line.decode(\"utf-8\").upper()\n",
    "                            text += \" \"\n",
    "            raw_text[zipfilename[:-4]] = text\n",
    "    text_data = process_text(raw_text)\n",
    "    with open('cache/processed_text.pickle', 'wb') as pfile:\n",
    "        pickle.dump(text_data, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain_number = {}\n",
    "for i, domain in enumerate(text_data.keys()):\n",
    "    domain_number[domain] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPNEWS247GREIDISEISPASOK\n",
      "ΠΕΜΠΤ\n",
      "13\n",
      "ΟΚΤΩΒΡ\n",
      "2016\n",
      "ΠΟΛΙΤ\n",
      "ΚΟΙΝΩΝ\n",
      "ΟΙΚΟΝΟΜ\n",
      "ΚΟΣΜ\n",
      "ΓΝΩΜ\n",
      "ΠΑΡΑΣΚΗΝΙ\n",
      "ΠΡΩΤΟΣΕΛΙΔ\n",
      "ΡΟΗ\n",
      "ΠΕΡΙΣΣ\n",
      "ΥΓΕΙ\n",
      "LIFE\n",
      "GUIDE\n",
      "ΤΑΞΙΔ\n",
      "VIRAL\n",
      "ΨΥΧΑΓΩΓ\n"
     ]
    }
   ],
   "source": [
    "import stemmer as st\n",
    "for word in text_data['news247.gr'].split()[:20]:\n",
    "    print process_word(word.upper())\n",
    "    #print st.stem(word.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import networkx as nx\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2683\n",
      "2041\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "\n",
    "edges = []\n",
    "with open('dataset/edgelist.txt', 'r')as fp:\n",
    "    for line in fp:\n",
    "        edges.append((line.split()[0], line.split()[1]))\n",
    "print len(edges)\n",
    "nodes = set([node for _tuple in edges for node in _tuple])\n",
    "print len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def compute_text_similarity(data):\n",
    "    vec = TfidfVectorizer(max_features=500)\n",
    "    x = vec.fit_transform(data)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2041, 500)\n"
     ]
    }
   ],
   "source": [
    "X_text = compute_text_similarity(text_data)\n",
    "print X_text.shape\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "X_text = cosine_similarity(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0169301810461\n"
     ]
    }
   ],
   "source": [
    "print X_text[domain_number['news247.gr'], domain_number['newsit.gr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_similarity(src, dst):\n",
    "    return X_text[domain_number[src], domain_number[dst]]  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_existent_edges = {}\n",
    "with open('dataset/not_existing_edges.txt', 'r')as fp:\n",
    "    for line in fp:\n",
    "        non_existent_edges[((line.split()[0], line.split()[1]))] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train= train_test_split(edges, test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2041\n",
      "2683\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pprint\n",
    "\n",
    "\"\"\"\n",
    "#G = nx.read_edgelist('dataset/edgelist.txt', delimiter='\\t', create_using=nx.DiGraph())\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(X_train)\n",
    "G.add_nodes_from(nodes)\"\"\"\n",
    "G = nx.read_edgelist('dataset/edgelist.txt', delimiter='\\t', create_using=nx.DiGraph())\n",
    "print(len(G.nodes()))\n",
    "print(len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4140197\n"
     ]
    }
   ],
   "source": [
    "non_edges = [edge for edge in list(nx.non_edges(G)) if not non_existent_edges.has_key(edge)]\n",
    "#4160957\n",
    "\n",
    "print len(non_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2683\n"
     ]
    }
   ],
   "source": [
    "sum_in_degree = sum(G.in_degree().values())\n",
    "print sum_in_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighborhood(src, dst):\n",
    "    neigh = {}\n",
    "    neigh['out_src'] = G.out_degree(src)/sum_in_degree\n",
    "    neigh['in_dst'] = G.in_degree(dst)/sum_in_degree\n",
    "    # maybe remove the next two features\n",
    "    neigh['in_src'] = G.in_degree(src)/sum_in_degree\n",
    "    neigh['out_dst'] = G.out_degree(dst)/sum_in_degree    \n",
    "    src_neigh = list(nx.all_neighbors(G, src))\n",
    "    dst_neigh = list(nx.all_neighbors(G, dst))\n",
    "    neigh['common_neighbors'] = len(set(src_neigh).intersection(dst_neigh))/sum_in_degree\n",
    "    out_dst = [a for _, a in G.out_edges(dst)]\n",
    "    in_dst = [a for a, _ in G.in_edges(dst)]\n",
    "    out_src = [a for _, a in G.out_edges(src)]    \n",
    "    in_src = [a for a, _ in G.in_edges(src)]\n",
    "    neigh['n_common_in'] = len(set(in_src).intersection(in_dst))/sum_in_degree\n",
    "    neigh['n_common_out'] = len(set(out_src).intersection(out_dst))/sum_in_degree\n",
    "    return neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pagerank = nx.pagerank(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_extraction(edge):\n",
    "    src, dst = edge\n",
    "    f_vector = []\n",
    "    f_vector.append(text_similarity(src, dst))\n",
    "    f_vector.append(pagerank[src])\n",
    "    #f_vector.append(pagerank[dst])\n",
    "    edge_hood = neighborhood(src, dst)\n",
    "    # add features: out degree of src, in degree of dst, # of common in with out\n",
    "    #f_vector.extend([len(edge_hood['out_src']), len(edge_hood['in_dst']), edge_hood['n_common_in'], edge_hood['n_common_out']])\n",
    "    f_vector.extend(edge_hood.values())\n",
    "    \n",
    "    return f_vector\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.00029761511829001677, 0.0003339730672479524, 0.0, 0.0, 0.0, 0.02981736861721953, 0.0007454342154304882, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "vector = feature_extraction(('efimerides.eu','cancer-society.gr'))\n",
    "print vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for edge in edges:\n",
    "    X_train.append(feature_extraction(edge))\n",
    "    y_train.append(1)\n",
    "for edge in non_existent_edges:\n",
    "    X_train.append(feature_extraction(edge))\n",
    "    y_train.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_predict = []\n",
    "for edge in non_edges:\n",
    "    X_predict.append(feature_extraction(edge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_n = np.array(X_train)\n",
    "y_train_n = np.array(y_train)\n",
    "X_predict_n = np.array(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23443, 10)\n",
      "(23443,)\n",
      "(4140197, 10)\n"
     ]
    }
   ],
   "source": [
    "print X_train_n.shape\n",
    "print y_train_n.shape\n",
    "print X_predict_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "reg = LogisticRegression()\n",
    "sgd = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "svm = SVC(probability=True)\n",
    "reg.fit(X_train_n, y_train_n)\n",
    "sgd.fit(X_train_n, y_train_n)\n",
    "svm.fit(X_train_n, y_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = reg.predict_proba(X_predict_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_sgd = sgd.predict_proba(X_predict_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_svm = svm.predict_proba(X_predict_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = []\n",
    "for t in pred_svm:\n",
    "    probs.append(t[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = sorted(range(len(probs)), key=lambda k: probs[k])[-453:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_edges = []\n",
    "for i in range(len(indices)-1, -1, -1):\n",
    "    index = indices[i]\n",
    "    predicted_edges.append(non_edges[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'agorazoome.gr', u'frontpages.gr'), (u'agorazoome.gr', u'google.gr'), (u'epirus-ellas.gr', u'frontpages.gr'), (u'epirus-ellas.gr', u'google.gr'), (u'portareto.gr', u'frontpages.gr')]\n"
     ]
    }
   ],
   "source": [
    "print predicted_edges[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "new_edges = []\n",
    "counter = 0\n",
    "while (counter<453):\n",
    "    if predicted_edges[counter][0]!='efimerides.eu' and predicted_edges[counter][0]!='pak-elta.gr':\n",
    "        new_edges.append(predicted_edges[counter])\n",
    "        counter+=1\n",
    "        print counter\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('predicted_edges_svm.txt', 'w') as fp:\n",
    "    for site_1, site_2 in predicted_edges:\n",
    "        fp.write(site_1+'\\t'+site_2+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
